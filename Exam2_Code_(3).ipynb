{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation,Dense,Dropout,Conv2D,Flatten,MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import optimizers\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "metadata": {
        "id": "qhdKQ06Uvsi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psoArKUvvuzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n"
      ],
      "metadata": {
        "id": "YzYVymnnr4Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "0ZffZgpUtt5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n"
      ],
      "metadata": {
        "id": "rKKdBgTft3fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "y_test_cat = to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "EjF88DiOt5sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train_cat,\n",
        "                    validation_data=(x_test, y_test_cat),\n",
        "                    epochs=10, batch_size=64, callbacks=[lr_reduction])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrT7BQmHr5GQ",
        "outputId": "d3a5031f-5beb-483d-a23d-e36219b91817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7569 - loss: 0.6988 - val_accuracy: 0.8757 - val_loss: 0.3397 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8753 - loss: 0.3591 - val_accuracy: 0.8878 - val_loss: 0.3040 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8923 - loss: 0.3039 - val_accuracy: 0.8933 - val_loss: 0.2817 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8995 - loss: 0.2807 - val_accuracy: 0.9019 - val_loss: 0.2688 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9052 - loss: 0.2580 - val_accuracy: 0.9030 - val_loss: 0.2624 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9090 - loss: 0.2463 - val_accuracy: 0.8998 - val_loss: 0.2681 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9127 - loss: 0.2374 - val_accuracy: 0.9069 - val_loss: 0.2508 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9176 - loss: 0.2187 - val_accuracy: 0.9051 - val_loss: 0.2524 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9213 - loss: 0.2079 - val_accuracy: 0.9082 - val_loss: 0.2527 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9326 - loss: 0.1792 - val_accuracy: 0.9137 - val_loss: 0.2397 - learning_rate: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Model creation function\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ehIRqIbHwrDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [5],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# Define a function to fit the model using GridSearchCV\n",
        "def fit_model(optimizer='adam', batch_size=32, epochs=5):\n",
        "    model = create_model(optimizer=optimizer)\n",
        "    history = model.fit(x_train, y_train_cat, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "    return history.history['accuracy'][-1]  # Return the final accuracy as the score for GridSearchCV\n",
        "\n",
        "# Manually perform GridSearchCV\n",
        "best_score = 0\n",
        "best_params = {}\n",
        "\n",
        "for optimizer in param_grid['optimizer']:\n",
        "    for batch_size in param_grid['batch_size']:\n",
        "        for epochs in param_grid['epochs']:\n",
        "            score = fit_model(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\n",
        "            print(f\"Score for optimizer={optimizer}, batch_size={batch_size}, epochs={epochs}: {score}\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'optimizer': optimizer, 'batch_size': batch_size, 'epochs': epochs}\n",
        "\n",
        "print(\"Best parameters found: \", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TipZqKcJwsie",
        "outputId": "61581f01-f926-4d19-c04d-2e3ae6b2b513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for optimizer=adam, batch_size=32, epochs=5: 0.8995500206947327\n",
            "Score for optimizer=adam, batch_size=64, epochs=5: 0.9023833274841309\n",
            "Score for optimizer=rmsprop, batch_size=32, epochs=5: 0.8951833248138428\n",
            "Score for optimizer=rmsprop, batch_size=64, epochs=5: 0.9003333449363708\n",
            "Best parameters found:  {'optimizer': 'adam', 'batch_size': 64, 'epochs': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5D6rgq3UsFL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TpD-JiE9sIDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ReduceLROnPlateau reduces the learning rate when validation loss stops improving, preventing stagnation.\n",
        "GridSearchCV finds the best optimizer  batch size, helping improve model generalization.\n",
        "- These steps reduce overfitting  improve convergence  accuracy.\n"
      ],
      "metadata": {
        "id": "fNkcL9fOsJHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "y_test_cat = to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "qSIjOSAqsRWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = np.zeros(10)\n",
        "class_total = np.zeros(10)\n",
        "for i in range(len(y_test)):\n",
        "    label = y_test[i]\n",
        "    pred = y_pred_classes[i]\n",
        "    if label == pred:\n",
        "        class_correct[label] += 1\n",
        "    class_total[label] += 1\n",
        "\n",
        "accuracy_per_class = class_correct / class_total\n",
        "plt.bar(range(10), accuracy_per_class)\n",
        "plt.title(\"Class-wise Accuracy\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ADp0gN6_sSVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "    ax.set_title(f\"True: {y_test[i]}, Pred: {y_pred_classes[i]}\")\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Sample Predictions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vYCgYqI_sVIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}